data:
  # path_to_trainfiles: "/data/ravinascos/LundNet/2024_graphs/graphs_NewDataset_LundNet_R22_ExtraNode_ln_kT_Cut_{ln_kT_cut}_LRJ_NewData_Primary"  # can also be a list of paths
  path_to_trainfiles: "/eos/home-t/tmlinare/Lund/Lund_tagging/lundtoptagger_tests_checks/combined_dataset_Wnonflat-2files_QCD_2percent"
  path_to_save: "/eos/home-t/tmlinare/Lund/Lund_tagging/lundtoptagger_data/models/"
  # model_name: "LundNet_R22_ExtraNode_No_ln_kT_Cut_LRJ_NewData_Primary"
  model_name: "LundNet_R22_ExtraNode_ln_kT_Cut_{ln_kT_cut}_LRJ_NewData_Primary"
  ln_kT_cut: 0 # null, -0.5, 0.0, 0.5, 1.0, 2.0, 2.8
  # if {ln_kT_cut} is present in path_to_trainfiles, path_to_save or model_name, it will be replaced by the value of ln_kT_cut (by None if ln_kT_cut is null)
  # the value from this config file and can be overriden by the command-line argument --ln_kT_cut

architecture:
  batch_size: 4500 #2600
  test_size: 0.1
  n_epochs: 35
  learning_rate: 0.0004 # 0.0005 for ln_kT_cut=2.8?
  choose_model: LundNet ## LundNet, GATNet, GINNet, EdgeGinNet, PNANet
  save_every_epoch: True

retrain:
  flag: False
  path_to_ckpt: "/sps/atlas/k/khandoga/TrainGNN/Models/LundNet_ufob_e012_0.12481.pt"

# choose gpu to run on
gpu: null # can be null or integer
# if null, code will check if CUDA is available and use torch.device("cuda") if it is, otherwise torch.device("cpu")
# Usually gpu 4 worked best, it had the most memory available (on UChicago?)
